# 智能计算系统2--第二章神经网络基础

+ 包含关系

  <img src=".\2-1.png" alt="2-1" style="zoom: 33%;" />

---

---

1. 从机器学习到神经网络

   + 机器学习相关概念

     + 机器学习是对能通过经验自动改进的计算机算法的研究（Mitchell）
     + 机器学习是用数据或以往的经验，以此提升计算机程序的能力（Plpaydin）
     + 机器学习是研究如何通过计算的手段、利用经验来改善系统自身性能的一门学科（周志华）

     ---

   + 典型机器学习过程

     <img src=".\2-2.png" alt="2-2" style="zoom: 50%;" />

     ---

   + 符号说明

     <img src=".\2-3.png" alt="2-3" style="zoom:50%;" />

     损失函数：计算真实值和计算值之间有多大差别的函数

     ---

   + 线性回归

     + 回归：分析因变量和自变量之间的关系

     + 线性回归：回归中最佳的一种关系，因变量和自变量是线性的

     + 问题引入

       <img src=".\2-4.png" alt="2-4" style="zoom: 80%;" />

     + 单变量线性回归模型（一元回归模型）

       线性回归可以找到一些点的集合背后的规律，一个点集可以用 一条直线来拟合，这条拟合出来的直线的参数特征，就是线性回归找到的点集背后的规律

       <img src=".\2-5.png" alt="2-5" style="zoom: 80%;" />

       <img src=".\2-6.png" alt="2-6" style="zoom:50%;" />

     + 多变量线性回归模型

       <img src=".\2-7.png" alt="2-7" style="zoom: 50%;" />

     + 线性函数拟合得好不好

       <img src=".\2-8.png" alt="2-8" style="zoom:50%;" />

       两个方法：最小二乘法；迭代法；

       <img src=".\2-9.png" style="zoom:50%;" />

     ---

   + 人工神经网络

     + 发展历程

       <img src=".\2-10.png" alt="2-10" style="zoom: 50%;" />

     + 人工神经元

       机器的学习领域，人工神经元是一个包含输入、输出和计算功能的模型

       不严格的说，其输入可类比为生物神经元的树突，输出可类别为神经元的轴突，计算可类比为神经元的细胞体

       <img src=".\2-11.png" alt="2-10" style="zoom:50%;" />

     + 感知机（Perceptron）模型**(最基础的神经网络模型)**

       <img src=".\2-12.png" alt="2-12" style="zoom:50%;" />

       sgin就相当于是激活函数

       + 感知机具体训练过程

         <img src=".\2-13.png" alt="2-13" style="zoom:50%;" />

         ||w||为w中成员平方之和再开根号

         <img src=".\2-14.png" alt="2-14" style="zoom:50%;" />

         <img src=".\2-15.png" alt="2-15" style="zoom:50%;" />

   ---

   ---

2. 神经网络训练

   + 两层神经网络（多层感知机）

     将大量的神经元模型进行组合，用不同的方法进行连接并作用在不同的激活函数上，就构成了人工神经网络模型

     多层感知机一般指全连接的两层神经网络模型

     <img src=".\2-16.png" alt="2-16" style="zoom:50%;" />

     <img src=".\2-17.png" alt="2-17" style="zoom:50%;" />

     <img src=".\2-18.png" alt="2-18" style="zoom:50%;" />

     <img src=".\2-19.png" alt="2-19" style="zoom:50%;" />

     共6+4+2=12个参数
     + 浅层神经网络特点
       + 需要数据量小，训练快
       + 局限性在于对复杂函数的表示能力有限，针对复杂分类问题其泛化能力受到制约
     + 为什么不做深层的：Kurt Hornik证明了理论上两层神经网络足以拟合任意函数，过去也没有足够的数据和计算能力

     ---

   + 多层神经网络（深度学习） --- 层数不断增加

     + 深度神经网络不断发展不仅依赖于自身的结构优势，也依赖于如下外在因素

       + Algorithm : 算法日新月异，优化算法层出不穷

         学习算法 --- BP算法 --- Pre-training ，Dropout等方法

       + Big data : 数据量不断增大

         10 --- 10k --- 100M

       + Computing : 处理器计算能力不断提升

         晶体管 --- CPU --- 集群/GPU --- 智能处理器

     + 三层神经网络

       <img src=".\2-20.png" alt="2-20" style="zoom:50%;" />

       第一层参数：2*3+1=7

       第二层参数：3*2+1=7

       第三层参数：3*3+1=10

       共24个参数

     + 随着网络层数的增加，每一层对于前一层次的抽象表示更深入，每一层神经元学习到的是前一层神经元更抽象的表示

     + 通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力

     + 多层神经网络

       <img src=".\2-21.png" alt="2-21" style="zoom:50%;" />

       <img src=".\2-22.png" alt="2-22" style="zoom:50%;" />

       

     ---

   + 神经网络的模型训练

     + 模型训练的目的，就是使得参数尽可能的与真实的模型逼近，模型计算值与真实值无限接近

     + 神经网络训练

       <img src=".\2-23.png" alt="2-23" style="zoom:50%;" />

       <img src=".\2-24.png" alt="2-24" style="zoom:50%;" />

       <img src=".\2-25.png" alt="2-25" style="zoom:50%;" />

       <img src=".\2-26.png" alt="2-26" style="zoom:50%;" />

     + 示例

       <img src=".\2-27.png" alt="2-27" style="zoom:50%;" />

       + 正向

         初始化权重值：随机设置

         <img src=".\2-28.png" alt="2-28" style="zoom:50%;" />

         输入到隐层计算

         <img src=".\2-29.png" alt="2-29" style="zoom:50%;" />

         隐层到输出层计算

         <img src=".\2-30.png" alt="2-30" style="zoom:50%;" />

       + 反向

         计算误差

         <img src=".\2-31.png" alt="2-31" style="zoom:50%;" />

         隐层到输出层的权值更新

         <img src=".\2-32.png" alt="2-32" style="zoom:50%;" />

         <img src=".\2-33.png" alt="2-33" style="zoom:50%;" />

         <img src=".\2-34.png" alt="2-34" style="zoom:50%;" />

         <img src=".\2-35.png" alt="2-35" style="zoom:50%;" />

     + 反向传播的作用

       将神经网络的输出误差反向传播到神经网络的输入端，并以此来更新神经网络中各个连接的权重

     + 反复训练过程

       当第一次反向传播完成后，网路的模型参数得到更新，网络进行下一轮的正向传播过程，如此反复的迭代进行训练，从而不断缩小计算值与真实值之间的误差

   ---

   ---

3. 神经网络设计原则（训练完了结果不准怎么办）

   + 网络的拓扑结构

     <img src=".\2-36.png" alt="2-36" style="zoom:50%;" />

     + 隐层的设计

       + 隐层节点的作用

         提取输入特征中的隐藏规律，每个节点赋予一定权重

       + 隐层节点数太少，则网络从样本中获取信息的能力就越差，无法反应数据集的规律
       + 隐层节点数太多，则网络的拟合能力过强，可能拟合数据集中的噪声部分，导致模型泛化能力变差

   + 激活函数

     <img src=".\2-37.png" alt="2-37" style="zoom: 67%;" />

     + sigmoid函数

       <img src=".\2-38.png" alt="2-38" style="zoom: 50%;" />

       对称点不是0，输出的平均值不是零，导致下一层输入值有偏移，影响神经网络的收敛

       饱和性问题：左右两边趋于平缓，当输入值非常大或非常小，算出来都非常接近于1或0，导致参数的更新会很慢

       梯度消失现象：sigmoid导数的取值范围有限，通常都是小于1的数字，导致随神经网络的层数的增加，梯度消失，尤其是靠近输入层的权重

     + tanh函数

       <img src=".\2-39.png" alt="2-39" style="zoom:50%;" />

     + ReLU函数

       <img src=".\2-40.png" alt="2-40" style="zoom:50%;" />

     + ReLU的改进版

       <img src=".\2-41.png" alt="2-41" style="zoom:50%;" />

     + ELU函数

       <img src=".\2-42.png" alt="2-42" style="zoom:50%;" />

   + 损失函数

     <img src=".\2-43.png" alt="2-43" style="zoom:50%;" />

     + 均方差损失函数

       <img src=".\2-44.png" alt="2-44" style="zoom:50%;" />

       <img src=".\2-45.png" alt="2-45" style="zoom:50%;" />

     

     + 交叉熵损失函数

       <img src=".\2-46.png" alt="2-46" style="zoom:50%;" />

       <img src=".\2-47.png" alt="2-47" style="zoom:50%;" />

     + 神经网络中损失函数的特性

       <img src=".\2-48.png" alt="2-48" style="zoom:50%;" />

   ---

   ---

4. 过拟合与正则化（效果还是不好怎么办）

   + 过拟合，泛化能力差

     <img src=".\2-49.png" alt="2-49" style="zoom:50%;" />

   + 解决方法：正则化

     <img src=".\2-50.png" alt="2-50" style="zoom:50%;" />

     + 正则化思路

       <img src=".\2-51.png" alt="2-51" style="zoom:50%;" />

       <img src=".\2-52.png" alt="2-52" style="zoom:50%;" />

       <img src=".\2-53.png" alt="2-53" style="zoom:50%;" />

     + L2正则化

       <img src=".\2-54.png" alt="2-54" style="zoom:50%;" />

     + L1正则化

       <img src=".\2-55.png" alt="2-55" style="zoom:50%;" />

     + Dropout正则化

       <img src=".\2-58.png" alt="2-58" style="zoom:50%;" />

       + 乘零的Dropout算法

         <img src=".\2-59.png" alt="2-59" style="zoom:50%;" />

         <img src=".\2-60.png" alt="2-60" style="zoom:50%;" />

     + Bagging集成方法

       <img src=".\2-56.png" alt="2-56" style="zoom:50%;" />

       <img src=".\2-57.png" alt="2-57" style="zoom:50%;" />

     + 其他正则化方法

       <img src=".\2-61.png" alt="2-61" style="zoom:50%;" />

   ---

   ---

5. 交叉验证

   交叉验证的方式给每个样本作为测试集和训练集的机会，充分利用样本信息，保证了鲁棒性，防止过拟合

   选择多种模型进行训练时，使用交叉验证能评判各模型的鲁棒性

   + 最简单的验证方式

     <img src=".\2-62.png" alt="2-62" style="zoom:50%;" />

   + Leave-one-out cross-validation验证方法

     <img src=".\2-63.png" alt="2-63" style="zoom:50%;" />

   + K-拆交叉验证（k-fold cross validation）

     <img src=".\2-64.png" alt="2-64" style="zoom:50%;" />